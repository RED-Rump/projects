{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "930ac41bb2c24f0d9abd4233765d9609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bc55d7e832e4c5e9d4a4e427bfe2749",
              "IPY_MODEL_36350ddbf0064c02b9a3ee0f2d1deaf1",
              "IPY_MODEL_64c4e65ef97f44509a9f4ab84cb9e31d"
            ],
            "layout": "IPY_MODEL_c4cb95eed6c84fb996aa80aa44cea329"
          }
        },
        "9bc55d7e832e4c5e9d4a4e427bfe2749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81f9a0e8e2c54f7a80248291d647d1db",
            "placeholder": "​",
            "style": "IPY_MODEL_bf6b8495c9f74ed2b3c065780fff19c0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "36350ddbf0064c02b9a3ee0f2d1deaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5200b5eb899b415a9a3130494e6d8210",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_223773c943674f1789f81edcdc24c495",
            "value": 48
          }
        },
        "64c4e65ef97f44509a9f4ab84cb9e31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36f708b925184e7b8896e2c2202be2b7",
            "placeholder": "​",
            "style": "IPY_MODEL_88f86538a0b54ba191d9569a4d8bcdaa",
            "value": " 48.0/48.0 [00:00&lt;00:00, 419B/s]"
          }
        },
        "c4cb95eed6c84fb996aa80aa44cea329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f9a0e8e2c54f7a80248291d647d1db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6b8495c9f74ed2b3c065780fff19c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5200b5eb899b415a9a3130494e6d8210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223773c943674f1789f81edcdc24c495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36f708b925184e7b8896e2c2202be2b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f86538a0b54ba191d9569a4d8bcdaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1851e3997033460d9661110b936031b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f97228bc1d7460d8d2617e3b51a386a",
              "IPY_MODEL_e491076f2d104db8ad85ed18330964d3",
              "IPY_MODEL_f158ef73067241f1a2387f09159e1ac7"
            ],
            "layout": "IPY_MODEL_703cd9d9c20c4475b0bbcbedc38976ae"
          }
        },
        "8f97228bc1d7460d8d2617e3b51a386a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_313c3eca33c24a84b44b683a07d89ecd",
            "placeholder": "​",
            "style": "IPY_MODEL_21d0f75d7dd74c7087cb3e5845f74967",
            "value": "config.json: 100%"
          }
        },
        "e491076f2d104db8ad85ed18330964d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b662e683c14e46d3bf374648a5ef6adc",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_958df010ebc04ad787a39e41b0593243",
            "value": 570
          }
        },
        "f158ef73067241f1a2387f09159e1ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baf7131ea6f54110b51bb32e84416102",
            "placeholder": "​",
            "style": "IPY_MODEL_7308f161923c4dbba8d7bfdd067375a8",
            "value": " 570/570 [00:00&lt;00:00, 6.80kB/s]"
          }
        },
        "703cd9d9c20c4475b0bbcbedc38976ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "313c3eca33c24a84b44b683a07d89ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21d0f75d7dd74c7087cb3e5845f74967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b662e683c14e46d3bf374648a5ef6adc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958df010ebc04ad787a39e41b0593243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "baf7131ea6f54110b51bb32e84416102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7308f161923c4dbba8d7bfdd067375a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccbf3b4963204b01b19874554696588d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5db3b54a78014d33946c7c9b0b686dd9",
              "IPY_MODEL_892ae755ae5241069e9de57b038df129",
              "IPY_MODEL_78b78de1fc3f4bb8ad3b7de7c72ed2a8"
            ],
            "layout": "IPY_MODEL_9a6898e300f84a0c9d493fedc27f5228"
          }
        },
        "5db3b54a78014d33946c7c9b0b686dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee0c8e58b0bc4c42b419018288aaf87d",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f6384ee5194850a76deae24d776a75",
            "value": "vocab.txt: 100%"
          }
        },
        "892ae755ae5241069e9de57b038df129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_600a2ddb7c764cdeb44a62094966cbd6",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25319be9aa7649c693aa80435bc475a0",
            "value": 231508
          }
        },
        "78b78de1fc3f4bb8ad3b7de7c72ed2a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68b6ab7911af46d9bdac0d5689b9cd47",
            "placeholder": "​",
            "style": "IPY_MODEL_832aeebf0cf44fef89599ed956b509ee",
            "value": " 232k/232k [00:00&lt;00:00, 1.74MB/s]"
          }
        },
        "9a6898e300f84a0c9d493fedc27f5228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee0c8e58b0bc4c42b419018288aaf87d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f6384ee5194850a76deae24d776a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "600a2ddb7c764cdeb44a62094966cbd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25319be9aa7649c693aa80435bc475a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68b6ab7911af46d9bdac0d5689b9cd47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "832aeebf0cf44fef89599ed956b509ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf1c767468048a09ffe3c08fbf6d573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06288bf99a504c95b8beaf122537126b",
              "IPY_MODEL_73479ec6f0b94b24b14e1c43377e60a4",
              "IPY_MODEL_e94bd7c6aac74bbfb3232b3758665ce7"
            ],
            "layout": "IPY_MODEL_f1487ec50e2240f5b2234d5c7dd13759"
          }
        },
        "06288bf99a504c95b8beaf122537126b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bebfed3d88ea4f0196537733bae59525",
            "placeholder": "​",
            "style": "IPY_MODEL_7a11919144a34f0ea9e76136e625f809",
            "value": "tokenizer.json: 100%"
          }
        },
        "73479ec6f0b94b24b14e1c43377e60a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fba5d7cc30ba4ddd938c92cd93b1b528",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b080bc5ff8944f9884fc20229138503",
            "value": 466062
          }
        },
        "e94bd7c6aac74bbfb3232b3758665ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d77b8a30dd42a39693100d39077e8c",
            "placeholder": "​",
            "style": "IPY_MODEL_6ddaeb8ce10040db93c91793572d085a",
            "value": " 466k/466k [00:00&lt;00:00, 5.15MB/s]"
          }
        },
        "f1487ec50e2240f5b2234d5c7dd13759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bebfed3d88ea4f0196537733bae59525": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a11919144a34f0ea9e76136e625f809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fba5d7cc30ba4ddd938c92cd93b1b528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b080bc5ff8944f9884fc20229138503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5d77b8a30dd42a39693100d39077e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ddaeb8ce10040db93c91793572d085a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72a93d83072448859f808798c0fc608e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_906c53f892df4de7b8e161688f5cc2ce",
              "IPY_MODEL_dbfaaa9fa5664c2dbaa9d08907176a55",
              "IPY_MODEL_85afdaaa731d49298827fecab4e973cd"
            ],
            "layout": "IPY_MODEL_80bbc7c3581843428fcf65c9e30a6975"
          }
        },
        "906c53f892df4de7b8e161688f5cc2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92d1f3a5a54e4d3dbd87c44a06b4ba70",
            "placeholder": "​",
            "style": "IPY_MODEL_aed52056c2284835991293b86a34ae9b",
            "value": "model.safetensors: 100%"
          }
        },
        "dbfaaa9fa5664c2dbaa9d08907176a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e029af208445b0993c5ceead1d3999",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7f548364ae04a0da694fc6c75970e35",
            "value": 440449768
          }
        },
        "85afdaaa731d49298827fecab4e973cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9bdf3cda0c74fe98ae60038819c126f",
            "placeholder": "​",
            "style": "IPY_MODEL_cff2ed6e57d34aaaa2402f5e8a289bd3",
            "value": " 440M/440M [00:06&lt;00:00, 49.2MB/s]"
          }
        },
        "80bbc7c3581843428fcf65c9e30a6975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92d1f3a5a54e4d3dbd87c44a06b4ba70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed52056c2284835991293b86a34ae9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e029af208445b0993c5ceead1d3999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f548364ae04a0da694fc6c75970e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9bdf3cda0c74fe98ae60038819c126f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff2ed6e57d34aaaa2402f5e8a289bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp5kWSpqvEcT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373d1466"
      },
      "source": [
        "# Task\n",
        "Build and fine-tune a BERT model for hate speech classification using the `HateSpeech_Kenya.csv` dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d36eef44"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the 'HateSpeech_Kenya.csv' dataset into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9d6fe09"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the pandas library to work with DataFrames, then load the specified CSV file into a DataFrame, and finally display the first few rows to verify the load.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2df86f7",
        "outputId": "7a8bd170-e0cd-49f4-85b6-14cccaf1b296"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Download the dataset if it's not already present\n",
        "!curl -O https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/main/dataset/HateSpeech_Kenya.csv\n",
        "\n",
        "df = pd.read_csv('HateSpeech_Kenya.csv')\n",
        "print(df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    14  100    14    0     0    117      0 --:--:-- --:--:-- --:--:--   118\n",
            "Empty DataFrame\n",
            "Columns: [404: Not Found]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd951f9b"
      },
      "source": [
        "## Load Dataset (Retry 1 - Corrected URL)\n",
        "\n",
        "### Subtask:\n",
        "Load the 'hatespeech_dataset.csv' dataset into a pandas DataFrame, correcting the download URL and filename.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7dc5b5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to download and load the dataset failed due to incorrect URLs and command line tool issues. I will try again using `wget` with the corrected URL and filename provided in the instructions to ensure the dataset is successfully downloaded and then loaded into a pandas DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f782515",
        "outputId": "2b3eb162-5d71-47c1-e508-c537fdd7826b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('HateSpeech_Kenya.csv')\n",
        "print(df.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [404: Not Found]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4967d06"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts failed because the specified file either didn't exist at the given URL or was corrupted due to a 404 error being saved as the file content. I will first remove any potentially corrupted existing file, then download the correct `HateSpeech_Kenya.csv` from its actual location in the GitHub repository, and finally load it into a pandas DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7a5ad53",
        "outputId": "651637bf-073b-48a9-b38e-b3e1e9fddf1e"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Define the (last known) URL for the dataset and the local filename\n",
        "url = 'https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/main/HateSpeech_Kenya.csv'\n",
        "local_filename = 'HateSpeech_Kenya.csv'\n",
        "\n",
        "# Remove any existing corrupted file to ensure a clean download attempt\n",
        "if os.path.exists(local_filename):\n",
        "    os.remove(local_filename)\n",
        "\n",
        "# Attempt to download the dataset using requests\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(f\"'{local_filename}' downloaded successfully.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n",
        "    # If download fails, create a dummy file to allow the notebook to proceed\n",
        "    if not os.path.exists(local_filename):\n",
        "        print(f\"Creating a dummy '{local_filename}' to proceed.\")\n",
        "        # Create a simple dummy DataFrame\n",
        "        dummy_data = {\n",
        "            'text': [\n",
        "                'This is a normal comment.',\n",
        "                'I hate this, it is terrible.',\n",
        "                'Love this, so good!',\n",
        "                'You are an idiot and should be banned.',\n",
        "                'Great work, keep it up!'\n",
        "            ],\n",
        "            'label': [0, 1, 0, 1, 0]\n",
        "        }\n",
        "        dummy_df = pd.DataFrame(dummy_data)\n",
        "        dummy_df.to_csv(local_filename, index=False)\n",
        "\n",
        "# Load the downloaded (or dummy) CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(local_filename)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading the file: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/main/HateSpeech_Kenya.csv\n",
            "Creating a dummy 'HateSpeech_Kenya.csv' to proceed.\n",
            "                                     text  label\n",
            "0               This is a normal comment.      0\n",
            "1            I hate this, it is terrible.      1\n",
            "2                     Love this, so good!      0\n",
            "3  You are an idiot and should be banned.      1\n",
            "4                 Great work, keep it up!      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee5f579c"
      },
      "source": [
        "## Preprocess Data\n",
        "\n",
        "### Subtask:\n",
        "Clean and preprocess the text data for BERT, including handling missing values and ensuring correct column names and label formats.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4c2249e"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to inspect the DataFrame's structure, column names, data types, and check for missing values to begin the preprocessing step as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f684ce33",
        "outputId": "069ce103-c26b-45d6-a687-a37e10de795d"
      },
      "source": [
        "print(df.info())\n",
        "print(df.isnull().sum())\n",
        "print(df.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    5 non-null      object\n",
            " 1   label   5 non-null      int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 212.0+ bytes\n",
            "None\n",
            "text     0\n",
            "label    0\n",
            "dtype: int64\n",
            "                                     text  label\n",
            "0               This is a normal comment.      0\n",
            "1            I hate this, it is terrible.      1\n",
            "2                     Love this, so good!      0\n",
            "3  You are an idiot and should be banned.      1\n",
            "4                 Great work, keep it up!      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d887592f"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the previous inspection, there are no missing values, the column names are correct, and the labels are in the desired numerical format. The next step is to check for and remove any duplicate text entries from the DataFrame to ensure data quality and avoid bias during model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48581571",
        "outputId": "7d0e95da-b273-4d2e-b0aa-07302cd671b1"
      },
      "source": [
        "initial_rows = len(df)\n",
        "df.drop_duplicates(subset=['text'], inplace=True)\n",
        "duplicates_removed = initial_rows - len(df)\n",
        "\n",
        "print(f\"Initial number of rows: {initial_rows}\")\n",
        "print(f\"Number of duplicate text entries removed: {duplicates_removed}\")\n",
        "print(f\"Number of rows after removing duplicates: {len(df)}\")\n",
        "print(df.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial number of rows: 5\n",
            "Number of duplicate text entries removed: 0\n",
            "Number of rows after removing duplicates: 5\n",
            "                                     text  label\n",
            "0               This is a normal comment.      0\n",
            "1            I hate this, it is terrible.      1\n",
            "2                     Love this, so good!      0\n",
            "3  You are an idiot and should be banned.      1\n",
            "4                 Great work, keep it up!      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6092b26"
      },
      "source": [
        "## Tokenize and encode text data\n",
        "\n",
        "### Subtask:\n",
        "Tokenize and encode the 'text' column using a pre-trained BERT tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5525b570"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to install the 'transformers' library, which provides the `AutoTokenizer` class necessary for BERT tokenization. This is a prerequisite before importing and using the tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9511a92",
        "outputId": "fdd3df20-b08b-45c9-9791-fdf4c5cfad89"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6626b663"
      },
      "source": [
        "**Reasoning**:\n",
        "With the `transformers` library installed, I will now import the `AutoTokenizer`, instantiate it with 'bert-base-uncased', tokenize the 'text' column of the DataFrame `df` with appropriate padding and truncation, and then print the shapes and a few samples of the tokenized outputs to verify the process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944,
          "referenced_widgets": [
            "930ac41bb2c24f0d9abd4233765d9609",
            "9bc55d7e832e4c5e9d4a4e427bfe2749",
            "36350ddbf0064c02b9a3ee0f2d1deaf1",
            "64c4e65ef97f44509a9f4ab84cb9e31d",
            "c4cb95eed6c84fb996aa80aa44cea329",
            "81f9a0e8e2c54f7a80248291d647d1db",
            "bf6b8495c9f74ed2b3c065780fff19c0",
            "5200b5eb899b415a9a3130494e6d8210",
            "223773c943674f1789f81edcdc24c495",
            "36f708b925184e7b8896e2c2202be2b7",
            "88f86538a0b54ba191d9569a4d8bcdaa",
            "1851e3997033460d9661110b936031b0",
            "8f97228bc1d7460d8d2617e3b51a386a",
            "e491076f2d104db8ad85ed18330964d3",
            "f158ef73067241f1a2387f09159e1ac7",
            "703cd9d9c20c4475b0bbcbedc38976ae",
            "313c3eca33c24a84b44b683a07d89ecd",
            "21d0f75d7dd74c7087cb3e5845f74967",
            "b662e683c14e46d3bf374648a5ef6adc",
            "958df010ebc04ad787a39e41b0593243",
            "baf7131ea6f54110b51bb32e84416102",
            "7308f161923c4dbba8d7bfdd067375a8",
            "ccbf3b4963204b01b19874554696588d",
            "5db3b54a78014d33946c7c9b0b686dd9",
            "892ae755ae5241069e9de57b038df129",
            "78b78de1fc3f4bb8ad3b7de7c72ed2a8",
            "9a6898e300f84a0c9d493fedc27f5228",
            "ee0c8e58b0bc4c42b419018288aaf87d",
            "b5f6384ee5194850a76deae24d776a75",
            "600a2ddb7c764cdeb44a62094966cbd6",
            "25319be9aa7649c693aa80435bc475a0",
            "68b6ab7911af46d9bdac0d5689b9cd47",
            "832aeebf0cf44fef89599ed956b509ee",
            "9cf1c767468048a09ffe3c08fbf6d573",
            "06288bf99a504c95b8beaf122537126b",
            "73479ec6f0b94b24b14e1c43377e60a4",
            "e94bd7c6aac74bbfb3232b3758665ce7",
            "f1487ec50e2240f5b2234d5c7dd13759",
            "bebfed3d88ea4f0196537733bae59525",
            "7a11919144a34f0ea9e76136e625f809",
            "fba5d7cc30ba4ddd938c92cd93b1b528",
            "6b080bc5ff8944f9884fc20229138503",
            "d5d77b8a30dd42a39693100d39077e8c",
            "6ddaeb8ce10040db93c91793572d085a"
          ]
        },
        "id": "caee83ae",
        "outputId": "0973c5aa-1d7b-40aa-da44-742f143f601e"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch # Import torch for tensor operations\n",
        "\n",
        "# Instantiate a tokenizer for a pre-trained BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the 'text' column\n",
        "encoded_data = tokenizer.batch_encode_plus(\n",
        "    df['text'].tolist(),\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128, # A common max length for BERT inputs\n",
        "    return_tensors='pt' # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "# Store the tokenized outputs\n",
        "input_ids = encoded_data['input_ids']\n",
        "attention_mask = encoded_data['attention_mask']\n",
        "# BERT base uncased typically doesn't use token_type_ids for single sentences, but we can check if it's there.\n",
        "# If it were a pair of sentences, token_type_ids would differentiate them.\n",
        "# For now, we'll assume single sentence input and only use input_ids and attention_mask.\n",
        "# If required, token_type_ids = encoded_data['token_type_ids'] would retrieve it if present.\n",
        "\n",
        "print(f\"Shape of input_ids: {input_ids.shape}\")\n",
        "print(f\"Shape of attention_mask: {attention_mask.shape}\")\n",
        "\n",
        "print(\"\\nFirst 2 tokenized samples (input_ids):\")\n",
        "print(input_ids[:2])\n",
        "print(\"\\nFirst 2 tokenized samples (attention_mask):\")\n",
        "print(attention_mask[:2])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "930ac41bb2c24f0d9abd4233765d9609"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1851e3997033460d9661110b936031b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccbf3b4963204b01b19874554696588d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cf1c767468048a09ffe3c08fbf6d573"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input_ids: torch.Size([5, 128])\n",
            "Shape of attention_mask: torch.Size([5, 128])\n",
            "\n",
            "First 2 tokenized samples (input_ids):\n",
            "tensor([[ 101, 2023, 2003, 1037, 3671, 7615, 1012,  102,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 101, 1045, 5223, 2023, 1010, 2009, 2003, 6659, 1012,  102,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "\n",
            "First 2 tokenized samples (attention_mask):\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "619e2a9f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully tokenized the text data. Now, I need to convert the 'label' column of the DataFrame into PyTorch tensors, which is the required format for model training, and then print its shape and the first few samples to confirm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e6503ff",
        "outputId": "bba701f2-3bb9-4e73-85f5-7192f3e29081"
      },
      "source": [
        "labels = torch.tensor(df['label'].values)\n",
        "\n",
        "print(f\"Shape of labels: {labels.shape}\")\n",
        "print(\"First 5 labels:\")\n",
        "print(labels[:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labels: torch.Size([5])\n",
            "First 5 labels:\n",
            "tensor([0, 1, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4889acd4"
      },
      "source": [
        "## Prepare Data for BERT\n",
        "\n",
        "### Subtask:\n",
        "Split the dataset into training and validation sets, and prepare PyTorch datasets suitable for BERT training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0e564f"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare the data for BERT training, I need to split the tokenized inputs and labels into training and validation sets using `train_test_split` and then create PyTorch `TensorDataset` and `DataLoader` objects for efficient batch processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33a7ba00",
        "outputId": "ae962455-9189-4ea8-8ae3-37c804812e45"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
        "    input_ids, labels, random_state=42, test_size=0.2\n",
        ")\n",
        "train_masks, validation_masks, _, _ = train_test_split(\n",
        "    attention_mask, labels, random_state=42, test_size=0.2\n",
        ")\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "validation_dataloader = DataLoader(validation_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(validation_data)}\")\n",
        "\n",
        "# Verify a sample batch from the training DataLoader\n",
        "print(\"\\nSample batch from training DataLoader:\")\n",
        "for batch in train_dataloader:\n",
        "    batch_input_ids, batch_attention_mask, batch_labels = batch\n",
        "    print(f\"  Input IDs shape: {batch_input_ids.shape}\")\n",
        "    print(f\"  Attention Mask shape: {batch_attention_mask.shape}\")\n",
        "    print(f\"  Labels shape: {batch_labels.shape}\")\n",
        "    print(f\"  First Input IDs in batch:\\n{batch_input_ids[0]}\")\n",
        "    print(f\"  First Label in batch: {batch_labels[0].item()}\")\n",
        "    break # Only show one batch"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 4\n",
            "Validation samples: 1\n",
            "\n",
            "Sample batch from training DataLoader:\n",
            "  Input IDs shape: torch.Size([4, 128])\n",
            "  Attention Mask shape: torch.Size([4, 128])\n",
            "  Labels shape: torch.Size([4])\n",
            "  First Input IDs in batch:\n",
            "tensor([ 101, 2307, 2147, 1010, 2562, 2009, 2039,  999,  102,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "  First Label in batch: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddf9fc00"
      },
      "source": [
        "## Load Pre-trained BERT Model and Tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Initialize a pre-trained BERT model for sequence classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9d9b82"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import `BertForSequenceClassification`, instantiate it for 'bert-base-uncased' with 2 labels, move it to the GPU if available, and then print its architecture to verify the setup.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900,
          "referenced_widgets": [
            "72a93d83072448859f808798c0fc608e",
            "906c53f892df4de7b8e161688f5cc2ce",
            "dbfaaa9fa5664c2dbaa9d08907176a55",
            "85afdaaa731d49298827fecab4e973cd",
            "80bbc7c3581843428fcf65c9e30a6975",
            "92d1f3a5a54e4d3dbd87c44a06b4ba70",
            "aed52056c2284835991293b86a34ae9b",
            "f2e029af208445b0993c5ceead1d3999",
            "a7f548364ae04a0da694fc6c75970e35",
            "f9bdf3cda0c74fe98ae60038819c126f",
            "cff2ed6e57d34aaaa2402f5e8a289bd3"
          ]
        },
        "id": "e146665d",
        "outputId": "4bfb8f6b-b732-4d88-980b-e66924d5a3ac"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Instantiate a pre-trained BERT model for sequence classification\n",
        "# num_labels=2 for binary classification (hate speech or not hate speech)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Determine the device to use (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model moved to: {device}\")\n",
        "print(\"BERT Model Architecture:\")\n",
        "print(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72a93d83072448859f808798c0fc608e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model moved to: cpu\n",
            "BERT Model Architecture:\n",
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "148eccfb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully initialized the BERT model. The next logical step is to define the training parameters such as the optimizer, learning rate scheduler, and set the number of training epochs, which are crucial for fine-tuning the BERT model for the hate speech classification task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56287046",
        "outputId": "f51c5362-02d2-4afe-8db5-c7be6b54245e"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW # Corrected import for AdamW\n",
        "\n",
        "# Set optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 4  # A common number of epochs for fine-tuning BERT\n",
        "\n",
        "# Total number of training steps\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0, # Default value in run_glue.py\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(\"Optimizer and scheduler configured.\")\n",
        "print(f\"Total training steps: {total_steps}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer and scheduler configured.\n",
            "Total training steps: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26373016"
      },
      "source": [
        "## Fine-tune BERT Model\n",
        "\n",
        "### Subtask:\n",
        "Configure and fine-tune the BERT model on the hate speech classification task using the prepared training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55041811"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import `tqdm` for progress bars and `numpy` for numerical operations. I also need to define a helper function to calculate accuracy for the evaluation step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6dd518e",
        "outputId": "d55cf069-cae1-45e6-c2c8-3070c2bd4aa4"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Function to calculate the accuracy metrics\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "print(\"Libraries imported and accuracy function defined.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and accuracy function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92467613"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the libraries are imported and the helper function is defined, I will proceed with the main fine-tuning loop for the BERT model, including training and validation phases, as per the detailed instructions provided in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96497500",
        "outputId": "e0fd9778-320e-4b1f-a683-5122e7081718"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss and accuracy values of each epoch\n",
        "history = []\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed:.2f}s.')\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the 'to' method.\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Clear any previously calculated gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different things depending on what parameters are given.\n",
        "        # `return_dict=True` returns a dataclass object. We want to access `loss` and `logits`.\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_attention_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\".\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = time.time() - t0\n",
        "\n",
        "    print(f'\\n  Average training loss: {avg_train_loss:.2f}')\n",
        "    print(f'  Training epoch took: {training_time:.2f}s')\n",
        "\n",
        "    print('\\nValidating...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the 'to' method.\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to build the computational graph during forward passes.\n",
        "        # This is a little faster and uses less memory, since we aren't doing\n",
        "        # backprop. Only necessary during evaluation/testing.\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # The \"logits\" are the output values prior to applying an activation function like softmax.\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                attention_mask=b_attention_mask,\n",
        "                labels=b_labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # add to the total accuracy.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(f'  Accuracy: {avg_val_accuracy:.2f}')\n",
        "\n",
        "    # Calculate the average loss over the validation data.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = time.time() - t0\n",
        "\n",
        "    print(f'  Validation Loss: {avg_val_loss:.2f}')\n",
        "    print(f'  Validation took: {validation_time:.2f}s')\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    history.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Validation Loss': avg_val_loss,\n",
        "            'Validation Accuracy': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print('\\nTraining complete!')\n",
        "print('Final training history:')\n",
        "for entry in history:\n",
        "    print(entry)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epoch took: 14.51s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.00\n",
            "  Validation Loss: 0.73\n",
            "  Validation took: 0.47s\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epoch took: 9.96s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.00\n",
            "  Validation Loss: 0.77\n",
            "  Validation took: 0.64s\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 8.35s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.00\n",
            "  Validation Loss: 0.79\n",
            "  Validation took: 0.63s\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epoch took: 6.81s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.00\n",
            "  Validation Loss: 0.80\n",
            "  Validation took: 0.49s\n",
            "\n",
            "Training complete!\n",
            "Final training history:\n",
            "{'epoch': 1, 'Training Loss': 0.6564269661903381, 'Validation Loss': 0.7313263416290283, 'Validation Accuracy': np.float64(0.0), 'Training Time': 14.507351636886597, 'Validation Time': 0.4658522605895996}\n",
            "{'epoch': 2, 'Training Loss': 0.6300721168518066, 'Validation Loss': 0.7737435102462769, 'Validation Accuracy': np.float64(0.0), 'Training Time': 9.956618070602417, 'Validation Time': 0.6415150165557861}\n",
            "{'epoch': 3, 'Training Loss': 0.5438485741615295, 'Validation Loss': 0.7924180626869202, 'Validation Accuracy': np.float64(0.0), 'Training Time': 8.352117538452148, 'Validation Time': 0.6338772773742676}\n",
            "{'epoch': 4, 'Training Loss': 0.5326117873191833, 'Validation Loss': 0.7967574596405029, 'Validation Accuracy': np.float64(0.0), 'Training Time': 6.811110496520996, 'Validation Time': 0.4900240898132324}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff4a3c8b"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the fine-tuned BERT model on the validation set using appropriate metrics such as accuracy, precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc7a9a14"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the model's performance on the validation set, I need to iterate through the validation data, make predictions, and collect true and predicted labels to compute detailed metrics like accuracy, precision, recall, and F1-score using `classification_report`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db0f5566",
        "outputId": "9de48a3b-828b-4a37-bf0f-718833d59afa"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "all_true_labels = []\n",
        "all_predicted_labels = []\n",
        "\n",
        "print('Evaluating on validation set...')\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in validation_dataloader:\n",
        "    # Unpack the batch and move to device\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_attention_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Tell pytorch not to build the computational graph during forward passes.\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_attention_mask\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Get predicted class (the one with the highest logit)\n",
        "    predicted_labels = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    all_predicted_labels.extend(predicted_labels)\n",
        "    all_true_labels.extend(label_ids)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_predicted_labels = np.array(all_predicted_labels)\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "\n",
        "# Print the classification report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(all_true_labels, all_predicted_labels))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on validation set...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       0.0\n",
            "           1       0.00      0.00      0.00       1.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d83e433"
      },
      "source": [
        "**Reasoning**:\n",
        "The current evaluation metrics are unreliable due to an extremely small and imbalanced validation set, which resulted from a failed attempt to download the actual dataset. The root cause is an incorrect URL used in an earlier step to retrieve 'HateSpeech_Kenya.csv'. I will correct the URL in the data loading cell (`e7a5ad53`) to ensure the real dataset is downloaded and used, which will then allow for a more meaningful train-validation split and valid evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c1e4667",
        "outputId": "13eba2d4-15a0-4c76-f334-8e44092ff082"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Define the (corrected) URL for the dataset and the local filename\n",
        "url = 'https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/main/dataset/HateSpeech_Kenya.csv'\n",
        "local_filename = 'HateSpeech_Kenya.csv'\n",
        "\n",
        "# Remove any existing corrupted file to ensure a clean download attempt\n",
        "if os.path.exists(local_filename):\n",
        "    os.remove(local_filename)\n",
        "\n",
        "# Attempt to download the dataset using requests\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(f\"'{local_filename}' downloaded successfully.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n",
        "    # If download fails, create a dummy file to allow the notebook to proceed\n",
        "    if not os.path.exists(local_filename):\n",
        "        print(f\"Creating a dummy '{local_filename}' to proceed.\")\n",
        "        # Create a simple dummy DataFrame\n",
        "        dummy_data = {\n",
        "            'text': [\n",
        "                'This is a normal comment.',\n",
        "                'I hate this, it is terrible.',\n",
        "                'Love this, so good!',\n",
        "                'You are an idiot and should be banned.',\n",
        "                'Great work, keep it up!'\n",
        "            ],\n",
        "            'label': [0, 1, 0, 1, 0]\n",
        "        }\n",
        "        dummy_df = pd.DataFrame(dummy_data)\n",
        "        dummy_df.to_csv(local_filename, index=False)\n",
        "\n",
        "# Load the downloaded (or dummy) CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(local_filename)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading the file: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/main/dataset/HateSpeech_Kenya.csv\n",
            "Creating a dummy 'HateSpeech_Kenya.csv' to proceed.\n",
            "                                     text  label\n",
            "0               This is a normal comment.      0\n",
            "1            I hate this, it is terrible.      1\n",
            "2                     Love this, so good!      0\n",
            "3  You are an idiot and should be banned.      1\n",
            "4                 Great work, keep it up!      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4632388b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to download the dataset still failed due to an incorrect URL. I will try a revised URL, changing 'main' to 'master' in the GitHub raw content link, as this is a common branch name for older repositories, and then attempt to download and load the dataset again. If this fails, I will provide a more substantial dummy dataset to ensure the subsequent steps can be properly executed with a larger and more representative dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3c79f39",
        "outputId": "e29560b1-ce62-4b31-81b3-61877dcfc4e7"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Define the (corrected) URL for the dataset and the local filename\n",
        "# Trying 'master' branch instead of 'main'\n",
        "url = 'https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/master/dataset/HateSpeech_Kenya.csv'\n",
        "local_filename = 'HateSpeech_Kenya.csv'\n",
        "\n",
        "# Remove any existing corrupted file to ensure a clean download attempt\n",
        "if os.path.exists(local_filename):\n",
        "    os.remove(local_filename)\n",
        "\n",
        "# Attempt to download the dataset using requests\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(f\"'{local_filename}' downloaded successfully.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n",
        "    # If download fails again, create a larger dummy file to allow the notebook to proceed with meaningful data\n",
        "    if not os.path.exists(local_filename):\n",
        "        print(f\"Creating a more substantial dummy '{local_filename}' to proceed.\")\n",
        "        # Create a more substantial dummy DataFrame for better testing\n",
        "        dummy_data = {\n",
        "            'text': [\n",
        "                'This is a normal comment and contains no hate speech.',\n",
        "                'I absolutely hate this person, they are disgusting.',\n",
        "                'What a wonderful day, filled with joy and positivity!',\n",
        "                'You are all pathetic losers and deserve nothing good.',\n",
        "                'This food is amazing, I love it very much.',\n",
        "                'Anyone who disagrees with me is a complete idiot.',\n",
        "                'Such a beautiful sunset, truly breathtaking.',\n",
        "                'These people are a plague on society, get rid of them!',\n",
        "                'Learning new things is always exciting and rewarding.',\n",
        "                'They should all be deported, they dont belong here.',\n",
        "                'Thank you for your kindness and support.',\n",
        "                'Your kind is a cancer and must be eradicated.',\n",
        "                'This movie was fantastic, highly recommend it!',\n",
        "                'I wish death upon all of them.',\n",
        "                'It was a pleasure working with you.',\n",
        "                'All immigrants are criminals, send them back!',\n",
        "                'The weather is perfect for a walk in the park.',\n",
        "                'You are a disgrace to humanity.',\n",
        "                'Enjoy your evening!',\n",
        "                'Those people are scum and should be eliminated.'\n",
        "            ],\n",
        "            'label': [\n",
        "                0, 1, 0, 1, 0,\n",
        "                1, 0, 1, 0, 1,\n",
        "                0, 1, 0, 1, 0,\n",
        "                1, 0, 1, 0, 1\n",
        "            ]\n",
        "        }\n",
        "        dummy_df = pd.DataFrame(dummy_data)\n",
        "        dummy_df.to_csv(local_filename, index=False)\n",
        "\n",
        "# Load the downloaded (or dummy) CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(local_filename)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading the file: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/master/dataset/HateSpeech_Kenya.csv\n",
            "Creating a more substantial dummy 'HateSpeech_Kenya.csv' to proceed.\n",
            "                                                text  label\n",
            "0  This is a normal comment and contains no hate ...      0\n",
            "1  I absolutely hate this person, they are disgus...      1\n",
            "2  What a wonderful day, filled with joy and posi...      0\n",
            "3  You are all pathetic losers and deserve nothin...      1\n",
            "4         This food is amazing, I love it very much.      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dac59cf4"
      },
      "source": [
        "**Reasoning**:\n",
        "Previous attempts to download the actual dataset failed with 404 errors, leading to the use of a dummy dataset. To perform a meaningful evaluation, I need to ensure the correct dataset is loaded. I will try downloading the dataset using `wget` from the correct GitHub raw URL as an alternative to `requests`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e55d92a",
        "outputId": "029d115a-fd3f-4368-f88f-5ec848dfc3fb"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the correct URL for the dataset and the local filename\n",
        "url = 'https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/main/HateSpeech_Kenya.csv'\n",
        "local_filename = 'HateSpeech_Kenya.csv'\n",
        "\n",
        "# Remove any existing file to ensure a clean download attempt\n",
        "if os.path.exists(local_filename):\n",
        "    os.remove(local_filename)\n",
        "\n",
        "# Attempt to download the dataset using !wget\n",
        "# This method is often more robust in Colab for GitHub raw links\n",
        "print(f\"Attempting to download '{local_filename}' from {url} using !wget...\")\n",
        "!wget -q $url -O $local_filename\n",
        "\n",
        "# Check if the download was successful\n",
        "if os.path.exists(local_filename) and os.path.getsize(local_filename) > 0:\n",
        "    print(f\"'{local_filename}' downloaded successfully.\")\n",
        "    # Load the downloaded CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(local_filename)\n",
        "    print(\"Dataset loaded from downloaded file.\")\n",
        "else:\n",
        "    print(f\"Download of '{local_filename}' failed or resulted in an empty file.\")\n",
        "    print(\"Using the previously created substantial dummy dataset.\")\n",
        "    # If download fails, create a more substantial dummy file to allow the notebook to proceed with meaningful data\n",
        "    dummy_data = {\n",
        "        'text': [\n",
        "            'This is a normal comment and contains no hate speech.',\n",
        "            'I absolutely hate this person, they are disgusting.',\n",
        "            'What a wonderful day, filled with joy and positivity!',\n",
        "            'You are all pathetic losers and deserve nothing good.',\n",
        "            'This food is amazing, I love it very much.',\n",
        "            'Anyone who disagrees with me is a complete idiot.',\n",
        "            'Such a beautiful sunset, truly breathtaking.',\n",
        "            'These people are a plague on society, get rid of them!',\n",
        "            'Learning new things is always exciting and rewarding.',\n",
        "            'They should all be deported, they dont belong here.',\n",
        "            'Thank you for your kindness and support.',\n",
        "            'Your kind is a cancer and must be eradicated.',\n",
        "            'This movie was fantastic, highly recommend it!',\n",
        "            'I wish death upon all of them.',\n",
        "            'It was a pleasure working with you.',\n",
        "            'All immigrants are criminals, send them back!',\n",
        "            'The weather is perfect for a walk in the park.',\n",
        "            'You are a disgrace to humanity.',\n",
        "            'Enjoy your evening!',\n",
        "            'Those people are scum and should be eliminated.'\n",
        "        ],\n",
        "        'label': [\n",
        "            0, 1, 0, 1, 0,\n",
        "            1, 0, 1, 0, 1,\n",
        "            0, 1, 0, 1, 0,\n",
        "            1, 0, 1, 0, 1\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(dummy_data)\n",
        "    df.to_csv(local_filename, index=False) # Overwrite existing dummy if wget failed\n",
        "    print(\"Dummy dataset re-created and loaded.\")\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download 'HateSpeech_Kenya.csv' from https://raw.githubusercontent.com/mugi-ken/HateSpeech-Detection-for-Kenyan-Languages/main/HateSpeech_Kenya.csv using !wget...\n",
            "Download of 'HateSpeech_Kenya.csv' failed or resulted in an empty file.\n",
            "Using the previously created substantial dummy dataset.\n",
            "Dummy dataset re-created and loaded.\n",
            "                                                text  label\n",
            "0  This is a normal comment and contains no hate ...      0\n",
            "1  I absolutely hate this person, they are disgus...      1\n",
            "2  What a wonderful day, filled with joy and posi...      0\n",
            "3  You are all pathetic losers and deserve nothin...      1\n",
            "4         This food is amazing, I love it very much.      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67155c3f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to download the actual dataset failed, and the `df` now contains a more substantial dummy dataset. To ensure the model is trained and evaluated on this larger dummy dataset, I need to re-execute the preprocessing steps. This involves inspecting the DataFrame's structure, checking for missing values, and removing duplicates, which are crucial prerequisites before tokenization and model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ed1603",
        "outputId": "5ab76abf-91e5-471f-8fb4-2c7089aeec0d"
      },
      "source": [
        "print(df.info())\n",
        "print(df.isnull().sum())\n",
        "print(df.head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20 entries, 0 to 19\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    20 non-null     object\n",
            " 1   label   20 non-null     int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 452.0+ bytes\n",
            "None\n",
            "text     0\n",
            "label    0\n",
            "dtype: int64\n",
            "                                                text  label\n",
            "0  This is a normal comment and contains no hate ...      0\n",
            "1  I absolutely hate this person, they are disgus...      1\n",
            "2  What a wonderful day, filled with joy and posi...      0\n",
            "3  You are all pathetic losers and deserve nothin...      1\n",
            "4         This food is amazing, I love it very much.      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "297626f0"
      },
      "source": [
        "**Reasoning**:\n",
        "After verifying the basic structure and absence of missing values in the newly loaded (dummy) DataFrame, the next preprocessing step is to check for and remove any duplicate text entries to ensure data quality and avoid bias during model training. This step was performed previously but needs to be re-executed for the new `df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb2295c8",
        "outputId": "fad8f3c5-16c9-4edd-e4cf-810b36739822"
      },
      "source": [
        "initial_rows = len(df)\n",
        "df.drop_duplicates(subset=['text'], inplace=True)\n",
        "duplicates_removed = initial_rows - len(df)\n",
        "\n",
        "print(f\"Initial number of rows: {initial_rows}\")\n",
        "print(f\"Number of duplicate text entries removed: {duplicates_removed}\")\n",
        "print(f\"Number of rows after removing duplicates: {len(df)}\")\n",
        "print(df.head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial number of rows: 20\n",
            "Number of duplicate text entries removed: 0\n",
            "Number of rows after removing duplicates: 20\n",
            "                                                text  label\n",
            "0  This is a normal comment and contains no hate ...      0\n",
            "1  I absolutely hate this person, they are disgus...      1\n",
            "2  What a wonderful day, filled with joy and posi...      0\n",
            "3  You are all pathetic losers and deserve nothin...      1\n",
            "4         This food is amazing, I love it very much.      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02373665"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the `df` DataFrame has been updated with a more substantial dummy dataset, I need to re-tokenize and encode the 'text' column using the pre-trained BERT tokenizer. This will ensure that the `input_ids` and `attention_mask` tensors are generated from the correct, larger dataset, which is crucial for subsequent model training and evaluation steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "216dbb4f",
        "outputId": "92d3fbe7-0043-4a5e-f7fc-19f8d508cef1"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch # Import torch for tensor operations\n",
        "\n",
        "# Instantiate a tokenizer for a pre-trained BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the 'text' column\n",
        "encoded_data = tokenizer.batch_encode_plus(\n",
        "    df['text'].tolist(),\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128, # A common max length for BERT inputs\n",
        "    return_tensors='pt' # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "# Store the tokenized outputs\n",
        "input_ids = encoded_data['input_ids']\n",
        "attention_mask = encoded_data['attention_mask']\n",
        "# BERT base uncased typically doesn't use token_type_ids for single sentences, but we can check if it's there.\n",
        "# If it were a pair of sentences, token_type_ids would differentiate them.\n",
        "# For now, we'll assume single sentence input and only use input_ids and attention_mask.\n",
        "# If required, token_type_ids = encoded_data['token_type_ids'] would retrieve it if present.\n",
        "\n",
        "print(f\"Shape of input_ids: {input_ids.shape}\")\n",
        "print(f\"Shape of attention_mask: {attention_mask.shape}\")\n",
        "\n",
        "print(\"\\nFirst 2 tokenized samples (input_ids):\")\n",
        "print(input_ids[:2])\n",
        "print(\"\\nFirst 2 tokenized samples (attention_mask):\")\n",
        "print(attention_mask[:2])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input_ids: torch.Size([20, 128])\n",
            "Shape of attention_mask: torch.Size([20, 128])\n",
            "\n",
            "First 2 tokenized samples (input_ids):\n",
            "tensor([[  101,  2023,  2003,  1037,  3671,  7615,  1998,  3397,  2053,  5223,\n",
            "          4613,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1045,  7078,  5223,  2023,  2711,  1010,  2027,  2024, 19424,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "First 2 tokenized samples (attention_mask):\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d471f8e0"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the `df` DataFrame now contains the updated dummy dataset, I need to re-convert the 'label' column into a PyTorch tensor. This ensures that the `labels` tensor is correctly aligned with the new data for subsequent steps like splitting and training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8df80130",
        "outputId": "4381b965-19af-44ea-9fbb-ac81283130bd"
      },
      "source": [
        "labels = torch.tensor(df['label'].values)\n",
        "\n",
        "print(f\"Shape of labels: {labels.shape}\")\n",
        "print(\"First 5 labels:\")\n",
        "print(labels[:5])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labels: torch.Size([20])\n",
            "First 5 labels:\n",
            "tensor([0, 1, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d893918c"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the `input_ids`, `attention_mask`, and `labels` have been updated with the larger dummy dataset, I need to re-execute the data splitting and DataLoader creation to ensure the training and validation sets are properly generated from the new data for BERT training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df09f484",
        "outputId": "f3767a61-d970-423b-8a6d-9b5b8086ef3b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
        "    input_ids, labels, random_state=42, test_size=0.2\n",
        ")\n",
        "train_masks, validation_masks, _, _ = train_test_split(\n",
        "    attention_mask, labels, random_state=42, test_size=0.2\n",
        ")\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "validation_dataloader = DataLoader(validation_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(validation_data)}\")\n",
        "\n",
        "# Verify a sample batch from the training DataLoader\n",
        "print(\"\\nSample batch from training DataLoader:\")\n",
        "for batch in train_dataloader:\n",
        "    batch_input_ids, batch_attention_mask, batch_labels = batch\n",
        "    print(f\"  Input IDs shape: {batch_input_ids.shape}\")\n",
        "    print(f\"  Attention Mask shape: {batch_attention_mask.shape}\")\n",
        "    print(f\"  Labels shape: {batch_labels.shape}\")\n",
        "    print(f\"  First Input IDs in batch:\\n{batch_input_ids[0]}\")\n",
        "    print(f\"  First Label in batch: {batch_labels[0].item()}\")\n",
        "    break # Only show one batch"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 16\n",
            "Validation samples: 4\n",
            "\n",
            "Sample batch from training DataLoader:\n",
            "  Input IDs shape: torch.Size([16, 128])\n",
            "  Attention Mask shape: torch.Size([16, 128])\n",
            "  Labels shape: torch.Size([16])\n",
            "  First Input IDs in batch:\n",
            "tensor([  101,  2115,  2785,  2003,  1037,  4456,  1998,  2442,  2022,  3690,\n",
            "        26022,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "  First Label in batch: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "124a7299"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the data preparation (tokenization, label conversion, train-validation split) has been re-executed with the updated (larger dummy) dataset, I need to re-initialize the pre-trained BERT model for sequence classification to ensure it is ready for fine-tuning on this new data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a4852f8",
        "outputId": "0e781c01-4462-4af1-ba3d-a4e8dafed8a7"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Instantiate a pre-trained BERT model for sequence classification\n",
        "# num_labels=2 for binary classification (hate speech or not hate speech)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Determine the device to use (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model moved to: {device}\")\n",
        "print(\"BERT Model Architecture:\")\n",
        "print(model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model moved to: cpu\n",
            "BERT Model Architecture:\n",
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "470c6a8d"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the model has been re-initialized, and the data loaders now reflect the larger dummy dataset, I need to re-configure the optimizer and scheduler to ensure the training parameters are correctly applied for fine-tuning the BERT model with the updated data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b38e4b7",
        "outputId": "f1cc9f73-de01-444c-e76a-2335a25441d3"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW # Corrected import for AdamW\n",
        "\n",
        "# Set optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 4  # A common number of epochs for fine-tuning BERT\n",
        "\n",
        "# Total number of training steps\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0, # Default value in run_glue.py\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(\"Optimizer and scheduler configured.\")\n",
        "print(f\"Total training steps: {total_steps}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer and scheduler configured.\n",
            "Total training steps: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1592c5b2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the libraries are imported and the helper function is defined, I will proceed with the main fine-tuning loop for the BERT model, including training and validation phases, as per the detailed instructions provided in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2c50459",
        "outputId": "e27df1b4-612b-44b0-856b-ff5c29213e26"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss and accuracy values of each epoch\n",
        "history = []\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed:.2f}s.')\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the 'to' method.\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Clear any previously calculated gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different things depending on what parameters are given.\n",
        "        # `return_dict=True` returns a dataclass object. We want to access `loss` and `logits`.\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_attention_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\".\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = time.time() - t0\n",
        "\n",
        "    print(f'\\n  Average training loss: {avg_train_loss:.2f}')\n",
        "    print(f'  Training epoch took: {training_time:.2f}s')\n",
        "\n",
        "    print('\\nValidating...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the 'to' method.\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to build the computational graph during forward passes.\n",
        "        # This is a little faster and uses less memory, since we aren't doing\n",
        "        # backprop. Only necessary during evaluation/testing.\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # The \"logits\" are the output values prior to applying an activation function like softmax.\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                attention_mask=b_attention_mask,\n",
        "                labels=b_labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # add to the total accuracy.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(f'  Accuracy: {avg_val_accuracy:.2f}')\n",
        "\n",
        "    # Calculate the average loss over the validation data.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = time.time() - t0\n",
        "\n",
        "    print(f'  Validation Loss: {avg_val_loss:.2f}')\n",
        "    print(f'  Validation took: {validation_time:.2f}s')\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    history.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Validation Loss': avg_val_loss,\n",
        "            'Validation Accuracy': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print('\\nTraining complete!')\n",
        "print('Final training history:')\n",
        "for entry in history:\n",
        "    print(entry)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 30.23s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.25\n",
            "  Validation Loss: 0.84\n",
            "  Validation took: 2.42s\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epoch took: 21.68s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.25\n",
            "  Validation Loss: 0.78\n",
            "  Validation took: 1.96s\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epoch took: 25.91s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.25\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 2.28s\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 25.52s\n",
            "\n",
            "Validating...\n",
            "  Accuracy: 0.25\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 1.73s\n",
            "\n",
            "Training complete!\n",
            "Final training history:\n",
            "{'epoch': 1, 'Training Loss': 0.6861403584480286, 'Validation Loss': 0.8399242162704468, 'Validation Accuracy': np.float64(0.25), 'Training Time': 30.227799892425537, 'Validation Time': 2.418818235397339}\n",
            "{'epoch': 2, 'Training Loss': 0.6358606815338135, 'Validation Loss': 0.7849396467208862, 'Validation Accuracy': np.float64(0.25), 'Training Time': 21.680530071258545, 'Validation Time': 1.9575185775756836}\n",
            "{'epoch': 3, 'Training Loss': 0.5855535864830017, 'Validation Loss': 0.7364221215248108, 'Validation Accuracy': np.float64(0.25), 'Training Time': 25.911731481552124, 'Validation Time': 2.2798783779144287}\n",
            "{'epoch': 4, 'Training Loss': 0.5689496397972107, 'Validation Loss': 0.7118344902992249, 'Validation Accuracy': np.float64(0.25), 'Training Time': 25.52401041984558, 'Validation Time': 1.7263741493225098}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "484f0167"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the model has been retrained with the larger dummy dataset, I need to re-evaluate its performance on the validation set using metrics like accuracy, precision, recall, and F1-score to assess its current state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d50dbf",
        "outputId": "4a476125-61ba-4fcc-9345-7bdcff864e4a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "all_true_labels = []\n",
        "all_predicted_labels = []\n",
        "\n",
        "print('Evaluating on validation set...')\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in validation_dataloader:\n",
        "    # Unpack the batch and move to device\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_attention_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Tell pytorch not to build the computational graph during forward passes.\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_attention_mask\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Get predicted class (the one with the highest logit)\n",
        "    predicted_labels = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    all_predicted_labels.extend(predicted_labels)\n",
        "    all_true_labels.extend(label_ids)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_predicted_labels = np.array(all_predicted_labels)\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "\n",
        "# Print the classification report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(all_true_labels, all_predicted_labels))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on validation set...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      1.00      0.40         1\n",
            "           1       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.25         4\n",
            "   macro avg       0.12      0.50      0.20         4\n",
            "weighted avg       0.06      0.25      0.10         4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9cd382a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous evaluation produced `UndefinedMetricWarning` due to `zero_division` because one of the classes had no predicted samples or true samples in the validation set. To suppress this warning and produce a cleaner report, I will set `zero_division=0` in `classification_report`, indicating that metrics for such classes should be treated as 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88e6e757",
        "outputId": "45e0509c-9211-4f55-e274-52df9a887fc0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "all_true_labels = []\n",
        "all_predicted_labels = []\n",
        "\n",
        "print('Evaluating on validation set...')\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in validation_dataloader:\n",
        "    # Unpack the batch and move to device\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_attention_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Tell pytorch not to build the computational graph during forward passes.\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            attention_mask=b_attention_mask\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Get predicted class (the one with the highest logit)\n",
        "    predicted_labels = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    all_predicted_labels.extend(predicted_labels)\n",
        "    all_true_labels.extend(label_ids)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_predicted_labels = np.array(all_predicted_labels)\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "\n",
        "# Print the classification report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(all_true_labels, all_predicted_labels, zero_division=0))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on validation set...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      1.00      0.40         1\n",
            "           1       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.25         4\n",
            "   macro avg       0.12      0.50      0.20         4\n",
            "weighted avg       0.06      0.25      0.10         4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ca7686"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the model's performance and discuss potential next steps or improvements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae84303c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Dataset Acquisition Failure:** The 'HateSpeech\\_Kenya.csv' dataset could not be downloaded from the specified GitHub URL, as all attempts (using `curl`, `wget`, and `requests`) consistently resulted in a \"404 Not Found\" error.\n",
        "*   **Reliance on Dummy Data:** Due to persistent download failures, a dummy dataset was generated and used for all subsequent steps. Initially, a very small dummy dataset (5 samples) was used, which was later replaced by a slightly larger, but still synthetic, dummy dataset of 20 samples to allow the process to continue.\n",
        "*   **Data Preprocessing and Preparation:**\n",
        "    *   The dummy dataset contained no missing values or duplicate text entries.\n",
        "    *   Text data was successfully tokenized and encoded using a `bert-base-uncased` tokenizer, producing `input_ids` and `attention_mask` tensors. Labels were converted to PyTorch tensors.\n",
        "    *   The dummy dataset was split into a training set (16 samples) and a validation set (4 samples).\n",
        "*   **Model Training and Performance (on Dummy Data):**\n",
        "    *   A `BertForSequenceClassification` model was initialized, and fine-tuned for 4 epochs.\n",
        "    *   Training loss showed a decreasing trend, from approximately 0.66 in Epoch 1 to 0.53 in Epoch 4.\n",
        "    *   However, the model's performance on the validation set was very poor. The validation accuracy for the larger dummy dataset was 0.25 across all epochs.\n",
        "    *   The classification report revealed an overall accuracy of 0.25. For the positive class (label 1), precision, recall, and F1-score were all 0.00, indicating that the model failed to correctly identify any instances of hate speech in the validation set, essentially predicting only class 0.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current model performance is not meaningful because it was trained and evaluated on a small, synthetic dummy dataset due to the inability to acquire the real dataset. The immediate next step is to obtain the actual 'HateSpeech\\_Kenya.csv' dataset or an alternative hate speech dataset.\n",
        "*   Once a real dataset is acquired, the entire pipeline, from data loading to model evaluation, should be re-executed to get a true assessment of the BERT model's performance on hate speech classification.\n"
      ]
    }
  ]
}